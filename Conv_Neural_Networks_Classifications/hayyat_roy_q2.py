# -*- coding: utf-8 -*-
"""hayyat_roy_q2.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1D7qZOmJggd6i2IKNX0s4ZaeC_mknIBTl
"""

# Roy Hayyat
# ITP 259 Fall 2023
# Final
# Q2

from keras.models import Sequential
from keras.layers import Dense, Dropout, Conv2D, Flatten, MaxPool2D
import numpy as np
import matplotlib.pyplot as plt
import pandas as pd
import random
from sklearn.model_selection import train_test_split

"""Write code to train a CNN model which classifies the facial expression dataset.  The images are 48 p x 48 p, grayscale.

1. Load the dataset from facialex.csv. Images are labeled in one of seven expressions.
"""

df = pd.read_csv("/content/facialex.csv")
class_names = ['Angry', 'Disgust', 'Fear', 'Happy', 'Sad', 'Surprise', 'Neutral']
df

"""2. Use random seed of 2023. (5)"""

random.seed(2023)

"""
3. Set X and y. (5)"""

X = df.iloc[:, 1:]
y = df.iloc[:, 0]
print(X.head())
print(y.head())

"""4. Visualize the first 25 images from the dataset. Label the images. (5)"""

plt.figure(figsize=[8, 8])

for i in range(30):
  plt.subplot(5, 6, i+1)
  picture = X.iloc[i]  # extracting the index and placing it in picture
  picture = np.array(picture)  # converting the picture into an array, and then reshaping to visualize the output
  picture = picture.reshape(48, 48)

  plt.imshow(picture, cmap='gray')  # render the picture directly as it is tensor
  plt.title(class_names[y[i]])  # mapping the index to get the actual letter
  plt.axis('off')

""" 5. Partition the dataset into train and test sets. Test partition 30%. . Stratify the partitions. Print the shapes of the train and test data sets. (5)"""

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=2023, stratify=y)

"""
6. Scale the pixel values"""

# Scale all excel values - dividing by 255 as images are in grayscale, from 0-255 to allow shading, which removes rough edges.
X_train = X_train / 255
X_test = X_test / 255

"""
7. Reshape X_train and X_test to prepare them for CNN.  (5)
Hint:
X_train = np.reshape(X_train.to_numpy(), (-1, 48, 48))
X_test = np.reshape(X_test.to_numpy(), (-1, 48,48))"""

X_train = np.reshape(X_train.to_numpy(), (-1, 48, 48))
X_test = np.reshape(X_test.to_numpy(), (-1, 48,48))
print("The shape of the feature train set is:", X_train.shape)
print("The shape of the target train set is:", y_train.shape)
print("The shape of the feature test set is:", X_test.shape)
print("The shape of the target test set is:", y_test.shape)

"""8.  Build a CNN sequence of layers. Must contain the following layers. Parameters are up to you. (5)
a.      At least 1 convolutional layer
b.      At least 1 dropout layer
c.       At least 1 maxpool layer
d.      At least 1 flatten layer
e.      At least 1 dense layer
"""

model = Sequential()

# The first conv layer’s neurons are not connected to every single pixel in the input layer, instead only
# to pixels in their receptive fields. (first layer of pixels stays square, not flattened).
model.add(Conv2D(50, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu', input_shape=(48, 48, 1)))
model.add(Conv2D(75, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu'))
# filter= 50  - the number of output filters in the convolution.
# kernel_size= specifying the height and width of the 2D convolution window.
# Strides= parameter dictating kernel movement across the input data.
# input_shape= check shape in variables

# MaxPool layer is added to shrink the input image, in order to reduce the computation size.
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Conv2D(75, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu'))

# One way to prevent overfitting in NN is to ignore randomly selected neurons during training
# In other words, “drop” some neurons. Here, we drop 25% of the neurons.
model.add(Dropout(0.25))
model.add(Conv2D(100, kernel_size=(3,3), strides=(1, 1), padding='same', activation='relu'))
model.add(MaxPool2D(pool_size=(2, 2)))
model.add(Dropout(0.25))

# After convolution/pooling, image gets smaller and deeper (more feature maps).
# Finally, a regular feedforward NN is added of fully connected layers (and ReLU).  These are flattened layers.
model.add(Flatten())
# Dense layers capture complex patterns in the data and learn the relationships between different parts of the input
model.add(Dense(500, activation='relu'))
model.add(Dropout(0.4))
model.add(Dense(250, activation='relu'))
model.add(Dropout(0.3))

# Final layer is the output layer with softmax. 7 possible classifications.
model.add(Dense(7, activation='Softmax'))

model.summary()

"""9. Use the loss function sparse_categorical_crossentropy when compiling the model

"""

model.compile(loss='sparse_categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

"""10.  Train the model with at least 2 epochs (5)

"""

history = model.fit(X_train, y_train, batch_size=64, epochs=3, validation_data=(X_test, y_test))

"""11.  Plot the loss and accuracy curves for both train and validation sets  (5)"""

plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.legend(['Training Loss', 'Validation Loss'])
plt.xlabel("Epochs")
plt.ylabel("Loss")
plt.title("Loss Curves")

plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.legend(['Training Accuracy', 'Validation Accuracy'])  # validation accuracy = test accuracy
plt.xlabel("Epochs")
plt.ylabel("Accuracy")
plt.title("Accuracy Curves")

"""12. Visualize the predicted and actual image labels for the first 25 images of the test partition (5)"""

y_pred = model.predict(X_test)
y_pred = np.argmax(y_pred, axis=1)  # turning y_pred from 2d to 1d

plt.figure(figsize= (14, 14))

for i in range (25):
  plt.subplot(5, 5, i+1)
  picture = X.iloc[i]  # extracting the index and placing it in picture
  picture = np.array(picture)  # converting the picture into an array, and then reshaping to visualize the output
  picture = picture.reshape(48, 48)
  plt.imshow(picture, cmap='gray')  # render the picture directly as it is tensor
  plt.title('True: %s, \nPredicted: %s' %
                (class_names[np.array(y_test)[i]], class_names[y_pred[i]]))
  plt.axis('off')

plt.subplots_adjust(hspace=1)

"""13.  Visualize the predicted and actual image labels for 25 random misclassified images. (5)

"""

# Dealing with tensorflow shape rather than DF. Therefore, approach is different:
all_failed_indices = []
iteration = 0

for i in y_test:  # finding all failed/misclassified indices
  if i != y_pred[iteration]: all_failed_indices.append(iteration)
  iteration = iteration + 1

plt.figure(figsize=(14, 14))

for j in range(25):
    plt.subplot(5, 5, j + 1)
    random = np.random.randint(0, len(all_failed_indices))  # pick a random failed prediction from the TF
    failed_index = all_failed_indices[random]

    failed_sample = X_test[failed_index]
    plt.imshow(failed_sample, cmap='gray')
    plt.title('True: %s, \nPredicted: %s' %
                (class_names[np.array(y_test)[failed_index]], class_names[y_pred[failed_index]]))
    plt.axis('off')
plt.show()